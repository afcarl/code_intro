{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping newspapers\n",
    "\n",
    "So far we've used APIs to access data on the internet. But sometimes we want to access data that doesn't have an API. How do we work around this?\n",
    "\n",
    "We can _scrape_ sites instead. This means writing a program that visits the webpage, sort of like you normally would, but can extract the information you want and return it to you directly.\n",
    "\n",
    "For example, what if you wanted to grab all recent New York Times articles to analyze how much they mention \"Trump\" vs \"climate\"? You could visit their page, click through all the links, search for those terms, and count them up. But that would be a ton of work.\n",
    "\n",
    "Or we could spend a bit of time and write a program to do that for us. Which is what we're going to do here.\n",
    "\n",
    "### The `newspaper` library\n",
    "\n",
    "We're going to use a Python library called `newspaper`. This provides us a way to very easily download articles from almost any news site without having to mess with the low-level guts of writing our own scrapers.\n",
    "\n",
    "\n",
    "## Scraping a single article\n",
    "\n",
    "Let's see how we can download a single New York Times article using `newspaper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests_cache\n",
    "requests_cache.install_cache('/tmp/newspaper.cache', backend='sqlite', expire_after=60*60*24*2) # expire after two days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import newspaper\n",
    "\n",
    "url = 'https://www.nytimes.com/2017/09/20/opinion/duterte-philippines.html'\n",
    "\n",
    "# use newspaper's Article class\n",
    "# and save the resulting data into a variable named `article`\n",
    "article = newspaper.Article(url)\n",
    "article.download()\n",
    "article.parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have access to all of the article's data:\n",
    "\n",
    "```python\n",
    "article.title\n",
    "article.authors\n",
    "article.top_image\n",
    "article.text\n",
    "article.publish_date\n",
    "article.meta_keywords\n",
    "```\n",
    "\n",
    "If we want to access even more data, we can have `newspaper` analyze the article for us by calling `article.nlp()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article.nlp()\n",
    "article.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get the counts of specific words in the article. You can refer to the Python Tips notebook for how to do this, but we'll also do it right here. Let's define a function so we can re-use this later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_into_words(article):\n",
    "    # First, we want to lower case and remove all punctuation from the article text.\n",
    "    # Because we don't want \"Hello!\" to be treated differently from \"hello\".\n",
    "    text = article.text.lower()\n",
    "    text = text.replace('.', ' ')\n",
    "    text = text.replace('!', ' ')\n",
    "    text = text.replace('?', ' ')\n",
    "    text = text.replace(\"'\", ' ')\n",
    "    text = text.replace(\"â€™\", ' ') # this is actually diff from the prev line\n",
    "    text = text.replace('\"', ' ')\n",
    "    text = text.replace(',', ' ')\n",
    "\n",
    "    # Now we can split it into words\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's try using it on the article we just downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = split_into_words(article)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now, to count the words, we'll use the Counter class\n",
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the count of an individual word, we can just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts['drugs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Scraping a news site\n",
    "\n",
    "Ok, so that's how we get one article. But we want to get as many as we can.\n",
    "\n",
    "Fortunately, `newspaper` provides a way to do that too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will grab as many NYT articles as it can from the main page\n",
    "nyt = newspaper.build('https://nytimes.com/', memoize_articles=False)\n",
    "len(nyt.articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The articles are available in `nyt.articles`; we can loop over them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for article in nyt.articles:\n",
    "    print(article.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This got all the article urls, but it did not yet the text. You can see for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyt.articles[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to download and parse each article like we did when we scraped the single article.\n",
    "\n",
    "We're dealing with the messiness of the internet here though, so some articles won't download properly. We only want to keep those that do.\n",
    "\n",
    "So what we'll do below is loop over each article and _try_ to download it, and if it doesn't work, we'll skip. We'll keep the articles that were successful in a list called `ok_articles`.\n",
    "\n",
    "When you run the code below, you may see lines saying \"You must `download()` an article before calling `parse()` on it!\". You can safely ignore these, this is `newspaper` warning us that the article wasn't downloaded correctly, but those are the ones we're not keeping.\n",
    "\n",
    "We'll write this as a function so we can re-use it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_articles(articles):\n",
    "    ok_articles = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            ok_articles.append(article)\n",
    "        except newspaper.ArticleException:\n",
    "            pass\n",
    "    return ok_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use it on the NYT articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ok_articles = download_articles(nyt.articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the articles in `ok_articles` should have text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ok_articles[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge #1\n",
    "\n",
    "Now see if you can get the counts for the words \"trump\" and \"climate\" across all of these articles.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- We've already defined a function that will take an article and give us back its words, `split_into_words`. Take advantage of that.\n",
    "- Remember when we're working with collections of data, e.g. a list of articles, we want to use `for` loops.\n",
    "- Also notice the pattern we've used a few times in this class, of looping over a list and collecting data into another list.\n",
    "    - If you want to dump an entire list (let's say it's called `a`) into another list (let's say it's called `b`), you can use `extend`. For example (you can try this out in a block below):\n",
    "    \n",
    "```python\n",
    "a = [0,1,2]\n",
    "b = [3,4,5]\n",
    "a.extend(b)\n",
    "print(a)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge #2\n",
    "\n",
    "Grab the articles from another news site using `newspaper` and get the counts for 'trump' and 'climate' on that page. How do those mentions compare to the NYT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
